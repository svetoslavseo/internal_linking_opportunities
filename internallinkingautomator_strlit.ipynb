{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpjMKi4Msx9yuzkjjMj2Jw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetoslavseo/internal_linking_opportunities/blob/main/internallinkingautomator_strlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXx5D2cwMA1H"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Streamlit App Title\n",
        "st.title(\"Google News Content Extractor\")\n",
        "\n",
        "# User Input Options\n",
        "st.sidebar.header(\"Input Options\")\n",
        "option = st.sidebar.radio(\"Choose input method:\", (\"Google News Sitemap\", \"Upload CSV\", \"Enter URLs Manually\"))\n",
        "\n",
        "# Session for requests\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"})\n",
        "\n",
        "# Function to fetch URLs from Sitemap\n",
        "def fetch_sitemap_urls(sitemap_url):\n",
        "    try:\n",
        "        response = session.get(sitemap_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        root = ET.fromstring(response.content)\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9',\n",
        "                     'news': 'http://www.google.com/schemas/sitemap-news/0.9'}\n",
        "        urls = []\n",
        "        for url_element in root.findall(\"ns:url\", namespace):\n",
        "            loc = url_element.find(\"ns:loc\", namespace)\n",
        "            if loc is not None:\n",
        "                urls.append(loc.text)\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to extract content from URL\n",
        "def extract_content(url, css_selector):\n",
        "    try:\n",
        "        response = session.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        element = soup.select_one(css_selector)\n",
        "        if element:\n",
        "            return element.get_text(separator=\" \", strip=True)\n",
        "        else:\n",
        "            return \"Content not found\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Collect URLs based on user selection\n",
        "urls = []\n",
        "if option == \"Google News Sitemap\":\n",
        "    sitemap_url = st.sidebar.text_input(\"Enter Google News Sitemap URL:\")\n",
        "    if st.sidebar.button(\"Fetch URLs\") and sitemap_url:\n",
        "        urls = fetch_sitemap_urls(sitemap_url)\n",
        "        st.sidebar.success(f\"Fetched {len(urls)} URLs from sitemap.\")\n",
        "\n",
        "elif option == \"Upload CSV\":\n",
        "    uploaded_file = st.sidebar.file_uploader(\"Upload CSV file with URLs\", type=[\"csv\"])\n",
        "    if uploaded_file is not None:\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "        if \"URL\" in df.columns:\n",
        "            urls = df[\"URL\"].dropna().tolist()\n",
        "            st.sidebar.success(f\"Loaded {len(urls)} URLs from CSV.\")\n",
        "        else:\n",
        "            st.sidebar.error(\"CSV must contain a column named 'URL'\")\n",
        "\n",
        "elif option == \"Enter URLs Manually\":\n",
        "    manual_urls = st.sidebar.text_area(\"Enter URLs (one per line):\")\n",
        "    if manual_urls:\n",
        "        urls = manual_urls.split(\"\\n\")\n",
        "        urls = [url.strip() for url in urls if url.strip()]\n",
        "        st.sidebar.success(f\"Loaded {len(urls)} URLs manually.\")\n",
        "\n",
        "# CSS Selector Input\n",
        "css_selector = st.text_input(\"Enter CSS Selector for extracting content:\")\n",
        "\n",
        "# Process URLs\n",
        "if st.button(\"Extract Content\") and urls and css_selector:\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = {executor.submit(extract_content, url, css_selector): url for url in urls}\n",
        "        for future in futures:\n",
        "            url = futures[future]\n",
        "            content = future.result()\n",
        "            results.append({\"URL\": url, \"Extracted Content\": content})\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    st.write(results_df)\n",
        "\n",
        "    # Provide download link for results\n",
        "    csv = results_df.to_csv(index=False).encode(\"utf-8\")\n",
        "    st.download_button(label=\"Download CSV\", data=csv, file_name=\"extracted_content.csv\", mime=\"text/csv\")\n"
      ]
    }
  ]
}